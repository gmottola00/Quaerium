"""
Composite generation evaluators.

Combines multiple generation metrics into a single evaluation.
"""

from __future__ import annotations

import logging
from typing import Any

from quaerium.core.evaluation.metrics import GenerationMetrics
from quaerium.infra.evaluation.generation.faithfulness import (
    FaithfulnessEvaluator,
    HallucinationDetector,
)
from quaerium.infra.evaluation.generation.llm_judge import OpenAIJudge
from quaerium.infra.evaluation.generation.relevance import (
    AnswerRelevanceEvaluator,
)

logger = logging.getLogger(__name__)


class StandardGenerationEvaluator:
    """
    Standard generation evaluator with all common metrics.

    Computes relevance, faithfulness, and hallucination scores
    using LLM-as-judge evaluation.

    Example:
        >>> judge = OpenAIJudge(llm=my_llm)
        >>> evaluator = StandardGenerationEvaluator(judge)
        >>> metrics = evaluator.evaluate_answer(
        ...     question="What is Python?",
        ...     generated_answer="Python is a programming language.",
        ...     context=["Python is a high-level programming language."]
        ... )
        >>> print(f"Relevance: {metrics.relevance_score:.2f}")
        >>> print(f"Faithfulness: {metrics.faithfulness_score:.2f}")
    """

    def __init__(self, judge: OpenAIJudge):
        """
        Initialize standard generation evaluator.

        Args:
            judge: LLM judge for evaluations
        """
        self.judge = judge
        self.relevance_evaluator = AnswerRelevanceEvaluator(judge)
        self.faithfulness_evaluator = FaithfulnessEvaluator(judge)
        self.hallucination_detector = HallucinationDetector(judge)

        logger.debug(
            f"Initialized generation evaluator with judge model: {judge.llm.model_name}"
        )

    def evaluate_answer(
        self,
        question: str,
        generated_answer: str,
        context: list[str],
        reference_answer: str | None = None,
        **kwargs: Any,
    ) -> GenerationMetrics:
        """
        Evaluate the quality of a generated answer.

        Args:
            question: Original question
            generated_answer: Answer generated by the RAG system
            context: Context chunks used for generation
            reference_answer: Optional ground truth answer (currently unused)
            **kwargs: Additional parameters for LLM judge

        Returns:
            GenerationMetrics with relevance, faithfulness, and hallucination scores

        Example:
            >>> metrics = evaluator.evaluate_answer(
            ...     question="How do I install Python?",
            ...     generated_answer="Download from python.org.",
            ...     context=["Visit python.org to download Python."],
            ...     reference_answer="Install from python.org"
            ... )
            >>> assert 0.0 <= metrics.relevance_score <= 1.0
            >>> assert 0.0 <= metrics.faithfulness_score <= 1.0
        """
        logger.debug(
            f"Evaluating answer for question: {question[:50]}... "
            f"(answer length: {len(generated_answer)}, context chunks: {len(context)})"
        )

        # Evaluate relevance
        relevance_score = self.relevance_evaluator.evaluate(
            question=question,
            answer=generated_answer,
            **kwargs,
        )

        # Evaluate faithfulness
        faithfulness_score = self.faithfulness_evaluator.evaluate(
            answer=generated_answer,
            context=context,
            **kwargs,
        )

        # Detect hallucinations
        hallucination_score = self.hallucination_detector.detect(
            answer=generated_answer,
            context=context,
            **kwargs,
        )

        # Calculate answer similarity if reference provided
        answer_similarity = None
        if reference_answer:
            # For now, this is a placeholder - could use embedding similarity
            # or another LLM-based comparison in the future
            logger.debug("Reference answer provided but similarity not yet implemented")

        # Create metrics object
        metrics = GenerationMetrics(
            relevance_score=relevance_score,
            faithfulness_score=faithfulness_score,
            hallucination_score=hallucination_score,
            answer_similarity=answer_similarity,
            metadata={
                "question": question,
                "answer_length": len(generated_answer),
                "num_context_chunks": len(context),
                "judge_model": self.judge.llm.model_name,
            },
        )

        logger.debug(
            f"Generation metrics: relevance={relevance_score:.3f}, "
            f"faithfulness={faithfulness_score:.3f}, "
            f"hallucination={hallucination_score:.3f}"
        )

        return metrics


__all__ = ["StandardGenerationEvaluator"]
